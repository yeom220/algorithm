>[!비선형 자료구조]
>데이터 요소가 순차적(Sequential)으로 또는 선형으로 배열되지 않는 자료구조를 비선형(Non-Linear) 자료구조라고 한다. 비선형 자료구조는 선형과 달리 멀티 레벨로 구성된다. 트리를 떠올리면 이해가 쉬운데, 이 때문에 탐색이 복잡하고 선형에 비해 구현하기도 다소 번잡한 편이지만, 메모리를 좀 더 효율적으로 활용할 수 있다는 장점이 있다. 대표적으로는 그래프를 예로 들 수 있으며, 그래프의 범주에 포함되는 트리 또한 비선형 자료구조다.
>

>[!그래프]
>수학에서, 좀 더 구체적으로 그래프 이론에서 그래프란 객체의 일부 쌍(pair)들이 '연관되어' 있는 객체 집합 구조를 말한다.

### 오일러 경로
오일러의 스케치를 현대식 그래프 구조에 따라 A부터 D까지를 정점(Vertex), a부터 g까지는 간선(Edge)으로 구성된 그래프라는 수학적 구조를 찾아볼 수 있다.
오일러는 모든 정점이 짝수 개의 차수(Degree)를 갖는다면 모든 다리를 한 번씩만 건너서 도달하는 것이 성립한다고 말했다. 칼 히어홀저가 이를 수학적으로 증명해낸다. 이를 '오일러의 정리(Euler's Theorem)'라 부른다. 아울러 모든 간선을 한 번씩 방문하는 유한 그래프(Finite Graph)를 일컬어 오일러 경로(Eulerian Trail/Eulerian Path)라 부른다.

### 해밀턴 경로
>해밀턴 경로는 각 정점을 한 번씩 방문하는 무향 또는 유향 그래프 경로를 말한다.

해밀턴 경로(Hamiltonian Path)와 오일러 경로의 차이점을 들자면, 오일러 경로는 간선을 기준으로 하고 해밀턴 경로는 정점을 기준으로 한다는 점이다. 그러나 이러한 단순한 차이에도 불구하고 놀랍게도 해밀턴 경로를 찾는 문제는 최적 알고리즘이 없는 대표적인 NP-완전(Complete) 문제다. (NP 문제 중 NP-난해(hard))인 문제를 NP-완전 문제라 부른다.)

원래의 출발점으로 돌아오는 경로는 특별히 해밀턴 순환이라 하는데, 이중에서도 특히 최단 거리를 찾는 문제는 알고리즘 분야에서는 외판원 문제(Travelling Salesman Problem)로도 유명하다. 외판원 문제란 각 도시를 방문하고 돌아오는 가장 짧은 경로를 찾는 문제, 즉 최단 거리인 해밀턴 순환을 찾는 문제이며, NP-난해 문제로 이론 컴퓨터과학 분야의 매우 중요한 문제 중 하나이기도 하다.
외판원 문제를 좀 더 살펴보자. 미국의 각 도시를 한 번씩 방문한다고 했을 때, 어떤 순서로 방문해야 가장 짧은 거리가 될까? 만약 도시가 20개라고 할 때 이 문제의 정답을 찾기 위해 다녀야 하는 총 경로의 수는 20!(`20! = 2,432,902,008,176,640,000`)다. 그러니까 약 240경 번의 경로를 다녀봐야 가장 짧은 경로를 찾을 수 있다. 문제는 단순하지만 정답은 실로 엄청나다.

>[!NP복잡도]
>NP는 비결정론적 튜링 기계(NTM)로 다항 시간 안에 풀 수 있는 판정 문제의 집합으로, NP는 비결정론적 다항시간(Non-deterministic Polynomial time)의 약자다.
>NP에 속하는 문제는 결정론적 튜링 기계로 다항 시간에 검증이 가능하고, 그 역도 성립한다. 또한 결정론적 튜링 기계로 다항 시간안에  풀 수 있는 문제는 비결정론적 튜링 기계로도 다항 시간안에 풀 수 있으므로, P 집합은 NP 집합의 부분집합이다. 이때 P가 NP의 진부분집합(Proper Subset)인지, 혹은 P와 NP가 같은지에 대해서는 아직 알려지지 않았다. 이 문제는 P-NP 문제로 불리우며 컴퓨터과학 분야의 대표적인 미해결 문제 중 하나다. 클레이 수학 연구소에서 발표한 7개의 밀레니엄 문제(미해결 문제) 중 하나이며 해결하는 사람에게 100만 달러의 상금이 걸려 있기도 하다.
>그런데 앞서 설명을 보면서 다소 혼동되는 부분이 한 군데 있을 것이다. 왜 해밀턴 경로 문제와 외판원 문제의 NP 문제가 다를까? 먼저, 혼동될 수 있으니 3가지 문제의 구분부터 다시 한번 살펴본다.
>- 해밀턴 경로: 한 번만 방문하는 경로
>- 해밀턴 순환: 한 번만 방문하여 출발지로 돌아오는 경로
>- 외판원 문제: 한 번만 방문하여 출발지로 돌아오는 경로 중 가장 짧은 경로
>따라서 이 세 문제는 '해밀턴 경로 > 해밀턴 순환 > 외판원 문제'의 포함 관계를 이룬다. 이제 해밀턴 경로와 외판원 문제, 2가지 문제를 다시 한번 정의해보자.
>1. 해밀턴 경로가 존재하는가?
>2. 각 도시를 방문하고 돌아오는 가장 짧은 경로를 찾아라.(최단 거리 해밀턴 순환을 찾아라.)
>
>이제, 이 두 문제에 어떤 차이가 있는지 판별할 수 있는가?
>다음으로 NP-완전 문제의 조건은 다음과 같다.
>- NP 문제다.
>- NP-난해 문제다.
>
>앞서 P-NP 문제는 컴퓨터과학 분야의 대표적인 미해결 문제 중 하나라고 했는데, 이에 따라 *P != NP* 인 경우와 *P == NP* 의 NP-완전, NP-난해 문제 집합에 대한 관계다. 참고로 *P != NP* 일 것이라고 많은 사람들이 추측만 할 뿐이며 아직 증명되지 않았다. 어느 경우이든 NP이면서 NP-난해 문제인 경우를 NP-완전 문제라 한다.
>앞서 1번 문제는 결정 문제(Decision Problem)로 다항 시간에 정답을 검증할 수 있다. 즉 NP 문제이며, NP-난해 문제다. NP-완전 문제의 조건에 부합하며 따라서 1번 문제는 NP-완전 문제다. 그러나 2번 문제는 결정 문제가 아니므로 NP 문제가 아닐 수 있다. 증명할 수 있는 NP-난해 문제이지만 NP 문제가 아닐 수 있으므로 NP-완전 문제가 아니다. 따라서 2번은 NP-난해 문제다.

외판원 문제는 다이나믹 프로그래밍 기법을 활용하면 좀 더 최적화할 수 있다. 이 경우 *O(n^2 2^n)* 로 최적화할 수 있는데, 앞서 n=20인 경우 419,430,400 로 여전히 엄청난 수치이긴 하지만 240경 번이었던 이전에 비해서는 훨씬 더 빠르게 계산할 수 있다.

### 그래프 순회
>그래프 순회란 그래프 탐색(Search)이라고도 불리우며 그래프의 각 정점을 방문하는 과정을 말한다.

그래프의 각 정점을 방문하는 그래프 순회(Graph Traversals)에는 크게 깊이 우선 탐색(Depth-First Search)과 너비 우선 탐색(Breadth-First Search)의 2가지 알고리즘이 있다. DFS는 일반적으로 BFS에 비해 더 널리 쓰인다. 코딩 테스트 시에도 대부분의 그래프 탐색은 DFS로 구현하게 될 것이다.
DFS는 주로 스택으로 구현하거나 재귀로 구현하며, 이후에 살펴볼 백트래킹을 통해 뛰어난 효용을 보인다. 반면, BFS는 주로 큐로 구현하며, 그래프의 최단 경로를 구하는 문제 등에 사용된다. 다익스트라 알고리즘으로 최단 경로를 찾는 문제에서 BFS로 구현하는 코드를 살펴보게 될 것이다.

그래프를 표현하는 방법에는 크게 인접 행렬(Adjacency Matrix)와 인접 리스트(Adjacency List)의 2가지 방법이 있다. 인접 리스트는 출발 노드를 키로, 도착 노드를 값으로 표현할 수 있다. 도착 노드는 여러 개가 될 수 있으므로 리스트 형태가 된다. 파이썬의 딕셔너리 자료형으로 다음과 같이 나타낼 수 있다.
```python
graph = {
	 1: [2, 3, 4],
	 2: [5],
	 3: [5],
	 4: [],
	 5: [6, 7],
	 6: [],
	 7: [3],
}
```
이 딕셔너리를 입력값으로 해서 각각 DFS, BFS를 구현해보고 어떤 결과가 나오는지 살펴본다.

### DFS(깊이 우선 탐색)
>일반적으로 DFS는 스택으로 구현하며, 재귀를 이용하면 좀 더 간단하게 구현할 수 있다.

**재귀 구조로 구현**
재귀를 이용한 DFS를 구현해본다. 먼저, 위키피디아에 제시된 수도코드는 다음과 같다.
```python
# 재귀를 이용한 DFS 구현 수도코드
DFS(G, v)
	label v as discovered
	for all directed edges from v to w that are in G.adjacentEdges(v) do
		if vertex w is not laveled as discovered then
			recursively call DFS(G, w)
```
이 수도코드에는 정점 `v`의 모든 인접 유향(Directed) 간선들을 반복하라고 표기되어 있다.
이 수도코드의 알고리즘을 동일하게 파이썬 코드로 구현해보면 다음과 같다.
```python
def recursive_dfs(v, discovered=[]):
	discovered.append(v)
	for w in graph[v]:
		if not w in discovered:
			discovered = recursive_dfs(w, discovered)
	return discovered
```
방문했던 정점, 즉 `discovered`를 계속 누적된 결과로 만들기 위해 리턴하는 형태만 받아오도록 처리했을 뿐 다른 부분들은, 수도코드와 맞춰서 작성해봤다. 위의 그래프를 입력값으로 한 탐색 결과는 다음과 같다.
```python
>>> f'recursive dfs: {recursive_dfs(1)}'
'recursive dfs: [1, 2, 5, 6, 7, 3, 4]'
```

**스택을 이용한 반복 구조로 구현**
```python
# 반복을 이용한 DFS 구현 수도코드
DFS-iterative(G, v)
	let S be a stack
	S.push(v)
	while S is not empty do
		v = S.pop()
		if v is not labeled as discovered then
			label v as discovered
			for all edges from v to w in G.adjacentEdges(v) do
				S.push(w)
```
스택을 이용해 모든 인접 간선을 추출하고 다시 도착점인 정점을 스택에 삽입하는 구조로 구현되어 있다. 파이썬 코드로 구현하면 다음과 같다.
```python
def iterative_dfs(start_v):
	discoverd = []
	stack = [start_v]
	while stack:
		v = stack.pop()
		if v not in discovered:
			discovered.append(v)
			for w in graph[v]:
				stack.append(w)
	return discovered
```
이와 같은 반복 구현은, 앞서 코드가 길고 빈틈없어 보이는 재귀 구현에 비해 우아함은 떨어지지만, 좀 더 직관적이라 이해하기는 훨씬 더 쉽다. 실행 속도 또한 더 빠른 편이다. 대부분의 경우 재귀 구현은 반복으로, 반복 구현은 재귀로 바꿔서 알고리즘을 구현 할 수 있다. 위의 그래프를 입력값으로 한 탐색 결과는 다음과 같다.
```python
>>> f'iterative dfs: {iterative_dfs(1)}'
'iterative dfs: [1, 4, 3, 5, 7, 6, 2]'
```

똑같은 DFS인데 순서가 다르다. 어떤 차이가 있을까? 재귀 DFS는 사전식 순서(Lexicographical Order)로 방문한 데 반해 반복 DFS는 역순으로 방문했다. 스택으로 구현하다 보니 가장 마지막에 삽입된 노드부터 꺼내서 반복하게 되고 이 경우 인접 노드에서 가장 최근에 담긴 노드, 즉 가장 마지막부터 방문하기 때문이다. 인접 노드를 한꺼번에 추가하는 형태이기 때문에, 자칫 BFS가 아닌가 하고 헷갈릴 수 있지만 깊이 우선으로 탐색한다는 점에서 DFS가 맞다. 만약 BFS라면 \[..., 4, 3, 5, ...] 순서가 아니라 \[..., 4, 3, 2, ...] 순서가 되어야 할 것이다.

### BFS(너비 우선 탐색)
>BFS는 DFS보다 쓰임새는 적지만 최단 경로를 찾는 다익스트라 알고리즘 등에 매우 유용하게 쓰인다.

**큐를 이용한 반복 구조로 구현**
BFS를 반복 구조로 구현할 때는 큐를 이용한다. 수도코드는 다음과 같다.
```python
# 큐를 이용한 BFS 수도코드
BFS(G, start_v)
	let Q be a queue
	label start_v as discovered
	Q.enqueue(start_v)
	while Q is not empty do
		v := Q.dequeue()
		if v is the goal then
			return v
		for all edges from v to w in G.adjacentEdges(v) do
			if w is not labeled as discovered then
				lavel w as discoverd
				w.parent := v
				Q.enqueue(w)
```
모든 인접 간선을 추출하고 도착점인 정점을 큐에 삽입하는 수도코드다. 이를 파이썬 코드로 구현하면 다음과 같다.
```python
def iterative_bfs(start_v):
	discovered = [start_v]
	queue = [start_v]
	while queue:
		v = queue.pop(0)
		for w in graph[v]:
			if w not in discovered:
				discovered.append(w)
				queue.append(w)
	return discovered
```
리스트 자료형을 사용했지만 `pop(0)`과 같은 큐의 연산만을 사용했다. 좀 더 최적화를 위해서는 데크 같은 자료형을 사용해 보는 것을 고민해볼 수 있다. 그래프를 입력값으로 한 탐색결과는 다음과 같다
```python
>>> f'iterative bfs:{iterative_bfs(1)}'
'iterative bfs: [1, 2, 3, 4, 5, 6, 7]'
```
BFS의 경우 단계별 차례인 숫자 순으로 실행됐으며, 1부터 순서대로 각각의 인접 노드를 우선으로 방문하는 너비 우선 탐색이 잘 실행됐음을 확인할 수 있다.

**재귀 구현 불가**
BFS를 재귀로 풀이해볼 수 있을까?
사실 많은 이가 혼동하는 부분인데 BFS는 재귀로 동작하지 않는다. 큐를 이용하는 반복 구현만 가능하다. 이 부분을 혼동하면 풀이에 어려움을 겪을 수 있다.

### 백트래킹
>백트래킹(Backtracking)은 해결책에 대한 후보를 구축해 나아가다 가능성이 없다고 판단되는 즉시 후보를 포기(백트랙 Backtrack)해 정답을 찾아가는 범용적인 알고리즘으로 제약 충족 문제(Constraint Satisfaction Problems)에 특히 유용하다.

백트래킹은 탐색을 하다가 더 갈 수 없으면 왔던 길을 되돌아가 다른 길을 찾는다는 데서 유래했다. 백트래킹은 DFS와 같은 방식으로 탐색하는 모든 방법을 뜻하며, DFS는 백트래킹의 골격을 이루는 알고리즘이다. 백트래킹은 주로 재귀로 구현하며, 알고리즘마다 DFS 변형이 조금씩 일어나지만 기본적으로 모두 DFS의 범주에 속한다. 백트래킹은 가보고 되돌아오고를 반복한다. 운이 좋으면 시행착오를 덜 거치고 목적지에 도착할 수 있지만 최악의 경우에는 모든 경우를 다 거친 다음에 도착할 수 있다. 이 때문에 브루트 포스와 유사하다. 하지만 한번 방문 후 가능성이 없는 경우에는 즉시 후보를 포기한다는 점에서 매번 같은 경로를 방문하는 브루트 포스보다는 훨씬 우아한 방식이라 할 수 있다.
브루트 포스로 전체 트리를 탐색한다면 매우 긴 시간이 소요된다. 하지만 DFS로 탐색을 시도하고, 가능성이 없는 후보는 즉시 포기하고 백트래킹한다면 트리의 불필요한 거의 대부분을 버릴 수 있다. 이를 트리의 가지치기(Pruning)라고 하며, 이처럼 불필요한 부분을 일찍 포기한다면 탐색을 최적화할 수 있기 때문에, 가지치기는 트리의 탐색 최적화 문제와도 관련이 깊다.

### 제약 충족 문제
백트래킹은 제약 충족 문제(Constraint Satisfaction Problems)를 풀이하는 데 필수적인 알고리즘이다. 앞서 살펴본 가지치기를 통해 제약 충족 문제를 최적화 하기 때문이다.
>제약 충족 문제란 수많은 제약 조건(Constraints)을 충족하는 상태(States)를 찾아내는 수학 문제를 일컫는다.

특히 제약 충족 문제는 인공지능이나 경영 과학 분야에서 심도 있게 연구되고 있으며, 합리적인 시간내에 문제를 풀기 위해 휴리스틱과 조합 탐색 같은 개념을 함께 결합해 문제를 풀이한다. 제약 충족 문제는 대표적으로 스도쿠(Sudoku)처럼 1에서 9까지 숫자를 한 번만 넣는(제약 조건 충족) 정답(상태)을 찾아내는 모든 문제 유형을 말한다. 스도쿠를 잘 풀이하려면 백트래킹을 하면서 가지치기를 통해 최적화하는 형태로 풀이할 수 있다.
스도쿠 외에도 십자말 풀이, 8퀸 문제, 4색 문제 같은 퍼즐 문제와 배낭 문제, 문자열 파싱, 조합 최적화 문제 등이 모두 제약 충족 문제에 속한다.

----
### 32. 섬의 개수
>1을 육지로, 0을 물로 가정한 2D 그리드 맵이 주어졌을때, 섬의 개수를 계산하라.
>(연결되어 있는 1의 덩어리 개수를 구하라.)

###### 1. DFS로 그래프 탐색
이 문제는 쾨니히스베르크의 다리 문제처럼 반드시 그래프 모양이 아니더라도 그래프형으로 변환해서 풀이할 수 있음을 확인해보는 좋은 문제다. 입력값이 정확히 그래프는 아니지만 사실상 동서남북이 모두 연결된 그래프로 가정하고 동일한 형태로 처리할 수 있으며, 네 방향 각각 DFS 재귀를 이용해 탐색을 끝마치면 1이 증가하는 형태로 육지의 개수를 파악할 수 있다.

먼저, 행렬 입력값인 grid의 행, 열 단위로 육지(1)인 곳을 찾아 진행하다가 육지를 발견하면 그때부터 `self.dfs()`를 호출해 탐색을 시작한다.
DFS 탐색을 하는 `dfs()` 함수는 동서남북을 모두 탐색하면서 재귀호출한다. 함수 상단에는 육지가 아닌 곳은 `return`으로 종료 조건을 설정해둔다. 이렇게 재귀 호출이 백트래킹으로 모두 빠져 나오면 섬 하나를 발견한 것으로 간주한다. 이때 이미 방문했던 곳은 1이 아닌 값으로 마킹한다. 즉 육지(1)를 더 이상 육지가 아닌 것으로 만든다. 그래야 다음에 다시 계산하는 경우가 생기지 않는다. 일종의 가지치기다. 간혹 또 다른 행렬을 생성해 그곳에 방문했던 경로를 저장하는 형태로 풀이하는 경우가 있는데, 이 문제는 그렇게 풀이할 필요가 없다. 곰곰이 생각해보면 현재의 행렬에 방문한 경로를 표시해두는 것으로도 충분하기 때문이다. 아울러 별도의 행렬을 생성할 경우 공간 복잡도가 *O(n)* 이 되기 때문에 공간의 활용 또한 비효율적이다.

dfs() 함수를 빠져 나온 후에는 해당 위치에서 탐색할 수 있는 모든 육지를 탐색한 것이므로, 카운트를 1 증가시킨다. 이제 입력값이 비어 있는 경우에 대해 예외처리를 포함한다.

>[!중첩 함수]
>중첩 함수(Nested Function)란 함수 내에 위치한 또 다른 함수로, 바깥에 위치한 함수들과 달리 부모 함수의 변수를 자유롭게 읽을 수 있다는 장점이 있다. 실무에서 자주 쓰이는 편은 아니지만 단일 함수로 해결해야 하는 경우가 잦은 코딩 테스트에서는 매우 자주 쓰이는 기능이다. 이 책에서도 대부분의 문제 풀이는 중첩 함수를 적극적으로 활용해 풀이한다. 중첩 함수가 부모 함수의 변수를 공유하는 예제는 다음과 같다.
> ```python
>def outer_function(t: str):
>	text: str = t
>
>	def inner_function():
>		print(text)
>		
>	inner_function()
>	
>outer_function('Hello!')
>----
>Hello!
>```
>
>여기서 `outer_function()`은 `inner_function()`을 호출했고, 아무런 파라미터도 넘기지 않았지만 부모 함수의 `text` 변수를 자유롭게 읽어들여 그 값인 `Hello!`를 출력했다. 이처럼 매번 파라미터를 전달하지 않아도 되기 때문에 구현이 깔끔해진다는 장점이 있다. 아울러 가변 객체인 경우 `append()`, `pop()`등 여러 가지 연산으로 조작도 가능하다. 그러나 재할당(=)이 일어날 경우 참조 ID가 변경되어 별도의 로컬 변수로 선언되므로 이 부분은 주의가 필요하다.
>
>**연산자 조작**
>중첩 함수에서 부모 함수에서 선언한 변수를 연산자로 조작하는 경우를 살펴본다.
>```python
>def outer_function(a: List[int]):
>	b: List[int] = a
>	print(id(b), b)
>	
>	def inner_function1():
>		b.append(4)
>		print(id(b), b)
>		
>	def inner_function2():
>		print(id(b), b)
>		
>	inner_function1()
>	inner_function2()
>	
>outer_function([1,2,3])
>-------
>4598336160 [1, 2, 3]
>4598336160 [1, 2, 3, 4]
>4598336160 [1, 2, 3, 4]
>```
>
>리스트는 가변 객체이며, 이처럼 중첩 함수내에서 `b.append(4)`와 같은 형태로 `append()` 메소드를 사용해 변수를 조작할 수 있다. 이렇게 조작된 값은 부모 함수에서도 그대로 동일하게 적용된다.
>
>**재할당**
>이번에는 재할당으로 참조 ID가 변경되는 경우를 살펴보자.
>```python
>def outer_function(t: str):
>	text: str = t
>	print(id(text), text)
>	
>	def inner_function1():
>		text = 'World!'
>		print(id(text), text)
>
>	def inner_function2():
>		print(id(text), text)
>		
>	inner_function1()
>	inner_function2()
>	
>outer_function('Hello!')
>-------
>4599124144 Hello!
>4599130288 World!
>4599124144 Hello!
>```
>
>여기서는 불변 객체인 문자형을 예로 들었다. 문자열은 불변 객체이기 때문에 조작할 수 없다. 값을 변경하려면 `text = 'World!'`와 같은 형태로 새롭게 재할당할 수밖에 없다. `=`연산자로 변수를 새롭게 할당하는 경우, 기존에 `4599124144`이었던 ID 값이 `4599130288`로 변경됨을 확인할 수 있다. 즉 참조 ID가 변경되어 서로 다른 변수가 된다. 중첩 함수인 경우에는 함수 내에서만 사용 가능한 새로운 로컬 변수로 선언되며, 여기서 수정된 값, 즉 재할당된 값은 부모 함수에서는 반영되지 않으므로 주의가 필요하다.
